{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T15:05:58.433778Z",
     "start_time": "2025-03-17T15:05:57.619861Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "pd.options.display.max_columns = None\n",
    "import joblib\n",
    "from itertools import chain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv', low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T13:14:27.007858900Z",
     "start_time": "2025-03-17T13:14:24.371145700Z"
    }
   },
   "id": "2ca21d159eb15b72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we fix the encoding to transform codes, such as 8 and 9 to NaN if they represent missing values. Also, we drop several columns that we found not present in the data dictionary, since we do not know what do they represent.\n",
    "\n",
    "Then we proceed with removing columns that contain > 20% of missing values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "376bf6597f036de3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial: 862\n",
      "remaining: 527\n",
      "dropped: 335\n"
     ]
    }
   ],
   "source": [
    "max_vals = df.select_dtypes(include='number').max()\n",
    "max_9 = max_vals[max_vals == 9].index.tolist()\n",
    "for col in max_9:\n",
    "    df.loc[df[col] == 9, col] = np.nan \n",
    "max_vals = df.select_dtypes(include='number').max()\n",
    "max_8 = max_vals[max_vals == 8].index.tolist()\n",
    "\n",
    "missing_subset = {'ALCFREQ', 'HATTMULT', 'STROKMUL', 'TIAMULT', 'ARTHTYPE', 'ARTHUPEX', 'ARTHLOEX', 'ARTHSPIN', 'ARTHUNK', 'CVDCOG', 'STROKCOG', 'CVDIMAG', 'CVDIMAG1', 'CVDIMAG2', 'CVDIMAG3', 'CVDIMAG4', 'PDNORMAL', 'SPEECH', 'FACEXP', 'TRESTRHD', 'TRESTLHD', 'TRESTRFT', 'TRESTLFT', 'TRACTRHD', 'TRACTLHD', 'RIGDNECK', 'RIGDUPRT', 'RIGDUPLF', 'RIGDLORT', 'RIGDLOLF', 'TAPSRT', 'TAPSLF', 'HANDMOVR', 'HANDMOVL', 'HANDALTR', 'HANDALTL', 'LEGRT', 'LEGLF', 'ARISING', 'POSTURE', 'GAIT', 'POSSTAB', 'BRADYKIN', 'RESTTRL', 'RESTTRR', 'SLOWINGL', 'SLOWINGR', 'RIGIDL', 'RIGIDR', 'BRADY', 'POSTINST', 'CORTDEF', 'SIVDFIND', 'CVDMOTL', 'CVDMOTR', 'CORTVISL', 'CORTVISR', 'SOMATL', 'SOMATR', 'EYEPSP', 'DYSPSP', 'AXIALPSP', 'GAITPSP', 'APRAXSP', 'APRAXL', 'APRAXR', 'CORTSENL', 'CORTSENR', 'ATAXL', 'ATAXR', 'ALIENLML', 'ALIENLMR', 'DYSTONL', 'DYSTONR', 'MYOCLLT', 'MYOCLRT', 'MOMOPARK', 'MOMOALS', 'AMNDEM', 'PCA', 'NAMNDEM', 'AMYLPET', 'AMYLCSF', 'FDGAD', 'HIPPATR', 'TAUPETAD', 'CSFTAU', 'FDGFTLD', 'TPETFTLD', 'MRFTLD', 'DATSCAN', 'IMAGLINF', 'IMAGLAC', 'IMAGMACH', 'IMAGMICH', 'IMAGMWMH', 'IMAGEWMH', 'CANCER', 'MYOINF', 'CONGHRT', 'AFIBRILL', 'HYPERT', 'ANGINA', 'HYPCHOL', 'VB12DEF', 'THYDIS', 'ARTH', 'ARTYPE', 'ARTUPEX', 'ARTLOEX', 'ARTSPIN', 'ARTUNKN', 'URINEINC', 'BOWLINC', 'SLEEPAP', 'REMDIS', 'HYPOSOM', 'SLEEPOTH', 'ANGIOCP', 'ANGIOPCI', 'PACEMAKE', 'HVALVE', 'ANTIENC'}\n",
    "cols_to_change = list(missing_subset.intersection(max_8))\n",
    "df[cols_to_change] = df[cols_to_change].replace(8, np.nan)\n",
    "max_vals = df.select_dtypes(include='number').max()\n",
    "max_8 = max_vals[max_vals == 8].index.tolist()\n",
    "\n",
    "df = df.drop(columns=['NPWBRF', 'NACCBRNN', 'NPGRCCA', 'NPGRLA', 'NPGRHA', 'NPGRSNH', 'NPGRLCH', 'NACCAVAS', 'NPTAN', 'NPABAN', 'NPASAN', 'NPTDPAN', 'NPTHAL', 'NACCBRAA', 'NACCNEUR', 'NPADNC', 'NACCDIFF', 'NACCAMY', 'NPINF', 'NACCINF', 'NPHEMO', 'NPHEMO1', 'NPHEMO2', 'NPHEMO3', 'NPOLD', 'NPOLD1', 'NPOLD2', 'NPOLD3', 'NPOLD4', 'NACCMICR', 'NPOLDD', 'NPOLDD1', 'NPOLDD2', 'NPOLDD3', 'NPOLDD4', 'NACCHEM', 'NACCARTE', 'NPWMR', 'NPPATH', 'NACCNEC', 'NPPATH2', 'NPPATH3', 'NPPATH4', 'NPPATH5', 'NPPATH6', 'NPPATH7', 'NPPATH8', 'NPPATH9', 'NPPATH10', 'NPPATH11', 'NACCLEWY', 'NPLBOD', 'NPNLOSS', 'NPHIPSCL', 'NPFTDTAU', 'NACCPICK', 'NPFTDT2', 'NACCCBD', 'NACCPROG', 'NPFTDT5', 'NPFTDT6', 'NPFTDT7', 'NPFTDT8', 'NPFTDT9', 'NPFTDT10', 'NPFTDTDP', 'NPALSMND', 'NPOFTD', 'NPOFTD1', 'NPOFTD2', 'NPOFTD3', 'NPOFTD4', 'NPOFTD5', 'NPTDPA', 'NPTDPB', 'NPTDPC', 'NPTDPD', 'NPTDPE', 'NPPDXA', 'NPPDXB', 'NACCPRIO', 'NPPDXD', 'NPPDXE', 'NPPDXF', 'NPPDXG', 'NPPDXH', 'NPPDXI', 'NPPDXJ', 'NPPDXK', 'NPPDXL', 'NPPDXM', 'NPPDXN', 'NPPDXP', 'NPPDXQ', 'NPARTAG', 'NPATGSEV', 'NPATGAMY', 'NPATGAM1', 'NPATGAM2', 'NPATGAM3', 'NPATGAM4', 'NPATGAM5', 'NPATGFRN', 'NPATGFR1', 'NPATGFR2', 'NPATGFR3', 'NPATGFR4'])\n",
    "\n",
    "initial = df.shape[1]\n",
    "threshold = 0.2 * len(df)\n",
    "df = df.dropna(thresh=threshold, axis=1)\n",
    "remaining = df.shape[1]\n",
    "dropped = initial - remaining\n",
    "\n",
    "print(f\"initial: {initial}\")\n",
    "print(f\"remaining: {remaining}\")\n",
    "print(f\"dropped: {dropped}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T13:14:30.494661700Z",
     "start_time": "2025-03-17T13:14:29.132377700Z"
    }
   },
   "id": "e003061a514429d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we apply the transformation mentioned during EDA - coalescing columns BILLS, SHOPPING, STOVE, TRAVEL into one column to reduce dimensionality\n",
    "At the end we also create a deep copy of the dataframe, for performance reasons, to aid with fragmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43bc71fa07121608"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "impairment_vars = ['BILLS', 'SHOPPING', 'STOVE', 'TRAVEL']\n",
    "\n",
    "functional_impairment = df[impairment_vars].sum(axis=1, skipna=True)\n",
    "\n",
    "df = pd.concat([df, functional_impairment.rename('FUNCTIONAL_IMPAIRMENT')], axis=1)\n",
    "df.drop(columns=impairment_vars, inplace=True)\n",
    "\n",
    "df = df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T13:14:31.927198800Z",
     "start_time": "2025-03-17T13:14:31.651510900Z"
    }
   },
   "id": "7a9fb885024d4cea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We follow by dropping the target variable - OUTCOME_EVENTMCI as well as TIME, since it is not a predictor\n",
    "Then we proceed by creating a training and testing datasets, split 80/20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c37a0ad39541c585"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X = df.drop(columns=['OUTCOME_EVENTMCI', 'TIME'])\n",
    "y = df['OUTCOME_EVENTMCI']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22, stratify=y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T13:14:33.158473100Z",
     "start_time": "2025-03-17T13:14:33.037514900Z"
    }
   },
   "id": "94b50a8d85dcd177"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can start building the preprocessing pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "defd1c435e4d26ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> First we create a transformer that is going to handle outliers. We will be using the transformer in the pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe0144d11dd22526"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class HandleOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_quantile=0.3, upper_quantile=0.7):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.quantile_bounds_ = {}\n",
    "        numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            Q1 = X[col].quantile(self.lower_quantile)\n",
    "            Q2 = X[col].quantile(self.upper_quantile)\n",
    "            IQR = Q2 - Q1\n",
    "            self.quantile_bounds_[col] = {\n",
    "                'lower_bound': Q1 - 1.5 * IQR,\n",
    "                'upper_bound': Q2 + 1.5 * IQR\n",
    "            }\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col not in self.quantile_bounds_:\n",
    "                continue  \n",
    "            bounds = self.quantile_bounds_[col]\n",
    "            mean_value = X[col].mean()\n",
    "            \n",
    "            X[col] = np.where(X[col] < bounds['lower_bound'], mean_value, \n",
    "                              np.where(X[col] > bounds['upper_bound'], mean_value, X[col]))\n",
    "        return X.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T13:14:34.888749Z",
     "start_time": "2025-03-17T13:14:34.877432700Z"
    }
   },
   "id": "e531247d64b4aa7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Now we can split the dataset to numerical and categorical columns and create two sub-pipelines and process them separately, in the end we will join them again\n",
    "\n",
    "> The numerical pipeline include a simple imputer using median, clipping outliers in the 0.3 and 0.7 quantiles and scaling the remaining values using standardscaler\n",
    "\n",
    "> The categorical pipeline is being imputed using most_frequent method, and the values are then encoded using one hot encoder.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82b2e37c88d7e1e5"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_train shape: (23738, 522)\n",
      "X_train shape after variance threshold of 0.01: (23738, 589)\n",
      "X_train shape after variance threshold of 0.05: (23738, 522)\n",
      "X_train shape after variance threshold of 0.1: (23738, 520)\n"
     ]
    }
   ],
   "source": [
    "num_cols = X.select_dtypes(['number']).columns\n",
    "cat_cols = X.select_dtypes(['object']).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('outlier', HandleOutliers(lower_quantile=0.3, upper_quantile=0.7)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "var_thresh_low = VarianceThreshold(threshold=0.01)\n",
    "var_thresh_mid = VarianceThreshold(threshold=0.05)\n",
    "var_thresh_high = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "vt_low_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('variance_threshold', var_thresh_low)\n",
    "])\n",
    "vt_mid_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('variance_threshold', var_thresh_mid)\n",
    "])\n",
    "vt_high_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('variance_threshold', var_thresh_high)\n",
    "])\n",
    "\n",
    "X_vt_low = vt_low_pipeline.fit_transform(X_train)\n",
    "X_vt_mid = vt_mid_pipeline.fit_transform(X_train)\n",
    "X_vt_high = vt_high_pipeline.fit_transform(X_train)\n",
    "\n",
    "print(\"Original X_train shape:\", X_train.shape)\n",
    "print(\"X_train shape after variance threshold of 0.01:\", X_vt_low.shape)\n",
    "print(\"X_train shape after variance threshold of 0.05:\", X_vt_mid.shape)\n",
    "print(\"X_train shape after variance threshold of 0.1:\", X_vt_high.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T14:21:08.935159800Z",
     "start_time": "2025-03-17T14:19:56.994409700Z"
    }
   },
   "id": "f5c5bbd3d0f66dab"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    k           score_func  n_features_rfe           rfe_estimator  \\\n",
      "0  50            f_classif              20      LogisticRegression   \n",
      "1  50            f_classif              20  RandomForestClassifier   \n",
      "2  50  mutual_info_classif              20      LogisticRegression   \n",
      "3  50  mutual_info_classif              20  RandomForestClassifier   \n",
      "4  20            f_classif              10      LogisticRegression   \n",
      "5  20            f_classif              10  RandomForestClassifier   \n",
      "6  20  mutual_info_classif              10      LogisticRegression   \n",
      "7  20  mutual_info_classif              10  RandomForestClassifier   \n",
      "\n",
      "   train_score  test_accuracy  \n",
      "0     0.974724       0.973547  \n",
      "1     0.974556       0.974221  \n",
      "2     0.974724       0.973547  \n",
      "3     0.974556       0.974221  \n",
      "4     0.931671       0.935973  \n",
      "5     0.931671       0.935973  \n",
      "6     0.931671       0.935973  \n",
      "7     0.931586       0.935973  \n"
     ]
    }
   ],
   "source": [
    "num_cols = X.select_dtypes(['number']).columns\n",
    "cat_cols = X.select_dtypes(['object']).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('outlier', HandleOutliers(lower_quantile=0.3, upper_quantile=0.7)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "score_funcs = [f_classif, mutual_info_classif]  \n",
    "n_features_rfe = [20, 10]  \n",
    "rfe_estimators = [LogisticRegression(max_iter=1000, solver='liblinear'), RandomForestClassifier()]  \n",
    "\n",
    "results = []\n",
    "\n",
    "for n_features in n_features_rfe:\n",
    "    k = 50 if n_features == 20 else 20\n",
    "\n",
    "    for score_func in score_funcs:\n",
    "        for estimator in rfe_estimators:\n",
    "            select_k = SelectKBest(score_func=score_func, k=k)\n",
    "            rfe = RFE(estimator=estimator, n_features_to_select=n_features)\n",
    "            full_pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('variance_threshold', var_thresh),\n",
    "                ('select_k_best', select_k),\n",
    "                ('rfe', rfe),\n",
    "                ('classifier', LogisticRegression(max_iter=1000, solver='liblinear'))\n",
    "            ])\n",
    "\n",
    "            full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "            train_score = full_pipeline.score(X_train, y_train)\n",
    "            test_predictions = full_pipeline.predict(X_test)\n",
    "            test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "            results.append({\n",
    "                'k': k,\n",
    "                'score_func': score_func.__name__,\n",
    "                'n_features_rfe': n_features,\n",
    "                'rfe_estimator': estimator.__class__.__name__,\n",
    "                'train_score': train_score,\n",
    "                'test_accuracy': test_accuracy,\n",
    "            })\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T15:13:59.332030400Z",
     "start_time": "2025-03-17T15:06:03.818242400Z"
    }
   },
   "id": "a7c8a3cdba5415dc"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9744291852725587, 0.9742207245155855, 'pipeline.pkl')"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = X.select_dtypes(['number']).columns\n",
    "cat_cols = X.select_dtypes(['object']).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('outlier', HandleOutliers(lower_quantile=0.3, upper_quantile=0.7)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "select_k = SelectKBest(score_func=mutual_info_classif, k=50)\n",
    "log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=20)\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('variance_threshold', var_thresh),\n",
    "    ('select_k_best', select_k),\n",
    "    ('rfe', rfe),\n",
    "    ('classifier', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_score = full_pipeline.score(X_train, y_train)\n",
    "test_predictions = full_pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "pipeline_filename = 'pipeline.pkl'\n",
    "joblib.dump(full_pipeline, pipeline_filename)\n",
    "\n",
    "train_score, test_accuracy, pipeline_filename"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T15:21:46.388158800Z",
     "start_time": "2025-03-17T15:20:02.268929100Z"
    }
   },
   "id": "a1949d7c0d85401f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
