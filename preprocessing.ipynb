{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T20:12:48.998877400Z",
     "start_time": "2025-03-12T20:12:48.850666800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as stats\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv', low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T20:12:50.848907700Z",
     "start_time": "2025-03-12T20:12:48.871671200Z"
    }
   },
   "id": "2ca21d159eb15b72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we fix the encoding to transform codes, such as 8 and 9 to NaN if they represent missing values. Also, we drop several columns that we found not present in the data dictionary, since we do not know what do they represent.\n",
    "\n",
    "Then we proceed with removing columns that contain > 20% of missing values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "376bf6597f036de3"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial: 862\n",
      "remaining: 527\n",
      "dropped: 335\n"
     ]
    }
   ],
   "source": [
    "max_vals = df.select_dtypes(include='number').max()\n",
    "max_9 = max_vals[max_vals == 9].index.tolist()\n",
    "for col in max_9:\n",
    "    df.loc[df[col] == 9, col] = np.nan \n",
    "max_vals = df.select_dtypes(include='number').max()\n",
    "max_8 = max_vals[max_vals == 8].index.tolist()\n",
    "\n",
    "missing_subset = {'ALCFREQ', 'HATTMULT', 'STROKMUL', 'TIAMULT', 'ARTHTYPE', 'ARTHUPEX', 'ARTHLOEX', 'ARTHSPIN', 'ARTHUNK', 'CVDCOG', 'STROKCOG', 'CVDIMAG', 'CVDIMAG1', 'CVDIMAG2', 'CVDIMAG3', 'CVDIMAG4', 'PDNORMAL', 'SPEECH', 'FACEXP', 'TRESTRHD', 'TRESTLHD', 'TRESTRFT', 'TRESTLFT', 'TRACTRHD', 'TRACTLHD', 'RIGDNECK', 'RIGDUPRT', 'RIGDUPLF', 'RIGDLORT', 'RIGDLOLF', 'TAPSRT', 'TAPSLF', 'HANDMOVR', 'HANDMOVL', 'HANDALTR', 'HANDALTL', 'LEGRT', 'LEGLF', 'ARISING', 'POSTURE', 'GAIT', 'POSSTAB', 'BRADYKIN', 'RESTTRL', 'RESTTRR', 'SLOWINGL', 'SLOWINGR', 'RIGIDL', 'RIGIDR', 'BRADY', 'POSTINST', 'CORTDEF', 'SIVDFIND', 'CVDMOTL', 'CVDMOTR', 'CORTVISL', 'CORTVISR', 'SOMATL', 'SOMATR', 'EYEPSP', 'DYSPSP', 'AXIALPSP', 'GAITPSP', 'APRAXSP', 'APRAXL', 'APRAXR', 'CORTSENL', 'CORTSENR', 'ATAXL', 'ATAXR', 'ALIENLML', 'ALIENLMR', 'DYSTONL', 'DYSTONR', 'MYOCLLT', 'MYOCLRT', 'MOMOPARK', 'MOMOALS', 'AMNDEM', 'PCA', 'NAMNDEM', 'AMYLPET', 'AMYLCSF', 'FDGAD', 'HIPPATR', 'TAUPETAD', 'CSFTAU', 'FDGFTLD', 'TPETFTLD', 'MRFTLD', 'DATSCAN', 'IMAGLINF', 'IMAGLAC', 'IMAGMACH', 'IMAGMICH', 'IMAGMWMH', 'IMAGEWMH', 'CANCER', 'MYOINF', 'CONGHRT', 'AFIBRILL', 'HYPERT', 'ANGINA', 'HYPCHOL', 'VB12DEF', 'THYDIS', 'ARTH', 'ARTYPE', 'ARTUPEX', 'ARTLOEX', 'ARTSPIN', 'ARTUNKN', 'URINEINC', 'BOWLINC', 'SLEEPAP', 'REMDIS', 'HYPOSOM', 'SLEEPOTH', 'ANGIOCP', 'ANGIOPCI', 'PACEMAKE', 'HVALVE', 'ANTIENC'}\n",
    "cols_to_change = list(missing_subset.intersection(max_8))\n",
    "df[cols_to_change] = df[cols_to_change].replace(8, np.nan)\n",
    "max_vals = df.select_dtypes(include='number').max()\n",
    "max_8 = max_vals[max_vals == 8].index.tolist()\n",
    "\n",
    "df = df.drop(columns=['NPWBRF', 'NACCBRNN', 'NPGRCCA', 'NPGRLA', 'NPGRHA', 'NPGRSNH', 'NPGRLCH', 'NACCAVAS', 'NPTAN', 'NPABAN', 'NPASAN', 'NPTDPAN', 'NPTHAL', 'NACCBRAA', 'NACCNEUR', 'NPADNC', 'NACCDIFF', 'NACCAMY', 'NPINF', 'NACCINF', 'NPHEMO', 'NPHEMO1', 'NPHEMO2', 'NPHEMO3', 'NPOLD', 'NPOLD1', 'NPOLD2', 'NPOLD3', 'NPOLD4', 'NACCMICR', 'NPOLDD', 'NPOLDD1', 'NPOLDD2', 'NPOLDD3', 'NPOLDD4', 'NACCHEM', 'NACCARTE', 'NPWMR', 'NPPATH', 'NACCNEC', 'NPPATH2', 'NPPATH3', 'NPPATH4', 'NPPATH5', 'NPPATH6', 'NPPATH7', 'NPPATH8', 'NPPATH9', 'NPPATH10', 'NPPATH11', 'NACCLEWY', 'NPLBOD', 'NPNLOSS', 'NPHIPSCL', 'NPFTDTAU', 'NACCPICK', 'NPFTDT2', 'NACCCBD', 'NACCPROG', 'NPFTDT5', 'NPFTDT6', 'NPFTDT7', 'NPFTDT8', 'NPFTDT9', 'NPFTDT10', 'NPFTDTDP', 'NPALSMND', 'NPOFTD', 'NPOFTD1', 'NPOFTD2', 'NPOFTD3', 'NPOFTD4', 'NPOFTD5', 'NPTDPA', 'NPTDPB', 'NPTDPC', 'NPTDPD', 'NPTDPE', 'NPPDXA', 'NPPDXB', 'NACCPRIO', 'NPPDXD', 'NPPDXE', 'NPPDXF', 'NPPDXG', 'NPPDXH', 'NPPDXI', 'NPPDXJ', 'NPPDXK', 'NPPDXL', 'NPPDXM', 'NPPDXN', 'NPPDXP', 'NPPDXQ', 'NPARTAG', 'NPATGSEV', 'NPATGAMY', 'NPATGAM1', 'NPATGAM2', 'NPATGAM3', 'NPATGAM4', 'NPATGAM5', 'NPATGFRN', 'NPATGFR1', 'NPATGFR2', 'NPATGFR3', 'NPATGFR4'])\n",
    "\n",
    "initial = df.shape[1]\n",
    "threshold = 0.2 * len(df)\n",
    "df = df.dropna(thresh=threshold, axis=1)\n",
    "remaining = df.shape[1]\n",
    "dropped = initial - remaining\n",
    "\n",
    "print(f\"initial: {initial}\")\n",
    "print(f\"remaining: {remaining}\")\n",
    "print(f\"dropped: {dropped}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T20:12:52.108598100Z",
     "start_time": "2025-03-12T20:12:50.857108500Z"
    }
   },
   "id": "e003061a514429d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we apply the transformation mentioned during EDA - coalescing columns BILLS, SHOPPING, STOVE, TRAVEL into one column to reduce dimensionality\n",
    "At the end we also create a deep copy of the dataframe, for performance reasons, to aid with fragmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43bc71fa07121608"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "impairment_vars = ['BILLS', 'SHOPPING', 'STOVE', 'TRAVEL']\n",
    "\n",
    "functional_impairment = df[impairment_vars].sum(axis=1, skipna=True)\n",
    "\n",
    "df = pd.concat([df, functional_impairment.rename('FUNCTIONAL_IMPAIRMENT')], axis=1)\n",
    "df.drop(columns=impairment_vars, inplace=True)\n",
    "\n",
    "df = df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T20:12:52.361391600Z",
     "start_time": "2025-03-12T20:12:52.095922800Z"
    }
   },
   "id": "7a9fb885024d4cea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can start building the preprocessing pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "defd1c435e4d26ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> First we create a transformer that is going to handle outliers. We will be using the transformer in the pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe0144d11dd22526"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class HandleOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_quantile=0.3, upper_quantile=0.7):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.quantile_bounds_ = {}\n",
    "        numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            Q1 = X[col].quantile(self.lower_quantile)\n",
    "            Q2 = X[col].quantile(self.upper_quantile)\n",
    "            IQR = Q2 - Q1\n",
    "            self.quantile_bounds_[col] = {\n",
    "                'lower_bound': Q1 - 1.5 * IQR,\n",
    "                'upper_bound': Q2 + 1.5 * IQR\n",
    "            }\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col not in self.quantile_bounds_:\n",
    "                continue  \n",
    "            bounds = self.quantile_bounds_[col]\n",
    "            mean_value = X[col].mean()\n",
    "            \n",
    "            X[col] = np.where(X[col] < bounds['lower_bound'], mean_value, \n",
    "                              np.where(X[col] > bounds['upper_bound'], mean_value, X[col]))\n",
    "        return X.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T20:22:21.599660500Z",
     "start_time": "2025-03-12T20:22:21.584399100Z"
    }
   },
   "id": "e531247d64b4aa7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Now we can split the dataset to numerical and categorical columns and create two sub-pipelines and process them separately, in the end we will join them again"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82b2e37c88d7e1e5"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_cols = df.select_dtypes(['number']).columns\n",
    "cat_cols = df.select_dtypes(['object']).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('outlier', HandleOutliers(lower_quantile=0.3, upper_quantile=0.7)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "df_transformed = preprocessor.fit_transform(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T20:22:51.346569400Z",
     "start_time": "2025-03-12T20:22:34.872173800Z"
    }
   },
   "id": "13576384b4c21897"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-12T20:12:53.974402700Z"
    }
   },
   "id": "473b198da1822239"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
